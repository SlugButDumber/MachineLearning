{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Face Recognition System"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from keras.preprocessing.image import ImageDataGenerator"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pre Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 116 images belonging to 3 classes.\n",
      "Found 23 images belonging to 3 classes.\n"
     ]
    }
   ],
   "source": [
    "# Training set\n",
    "\n",
    "train_datagen = ImageDataGenerator(\n",
    "    rescale=1./255, #for standardising\n",
    "    shear_range=0.2,\n",
    "    zoom_range=0.2,\n",
    "    horizontal_flip=True)\n",
    "\n",
    "training_set = train_datagen.flow_from_directory(\n",
    "    'dataset/training_set', #dataset folder path\n",
    "    target_size=(64,64), #target size (pixel, pixel)\n",
    "    batch_size=32,\n",
    "    class_mode = 'categorical' #binary or categorical output\n",
    ")\n",
    "\n",
    "# Test set\n",
    "\n",
    "test_datagen = ImageDataGenerator(\n",
    "    rescale=1./255 #no other parameter cause it be test set but still need to festure scale\n",
    ")\n",
    "\n",
    "test_set = test_datagen.flow_from_directory(\n",
    "    'dataset/test_set',\n",
    "    target_size=(64,64),\n",
    "    batch_size=32,\n",
    "    class_mode='categorical'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Anit': 0, 'Arhat': 1, 'Vivek': 2}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_set.class_indices #to check which class is assigned to which index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 'Anit', 1: 'Arhat', 2: 'Vivek'}\n"
     ]
    }
   ],
   "source": [
    "Mapping = {}\n",
    "for faceValue,faceName in zip(test_set.class_indices.values(),test_set.class_indices.keys()):\n",
    "    Mapping[faceValue]=faceName\n",
    "\n",
    "print(Mapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n"
     ]
    }
   ],
   "source": [
    "OutputNeurons = len(Mapping)\n",
    "print(OutputNeurons)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialising the CNN\n",
    "\n",
    "cnn = tf.keras.models.Sequential()\n",
    "\n",
    "#Convolution and max Pooling layer 1\n",
    "\n",
    "cnn.add(tf.keras.layers.Conv2D(filters=32, #number of filters\n",
    "                               kernel_size=3, #size of each filter\n",
    "                               activation='relu', #activation function\n",
    "                               input_shape=[64,64,3])) #64 by 64 is the size & For coloured images, for B/W [64,64,1]\n",
    "\n",
    "cnn.add(tf.keras.layers.MaxPool2D(pool_size=2, #to set the size of the window used for pool\n",
    "                                  strides=2)) #to set the number of pixels by which the window shifts per iteration\n",
    "\n",
    "#Convolution and max Pooling layer 2\n",
    "\n",
    "cnn.add(tf.keras.layers.Conv2D(filters=32, #number of filters\n",
    "                               kernel_size=3, #size of each filter\n",
    "                               activation='relu', #activation function\n",
    "                               )) #input shape is not required for any layer other than first\n",
    "\n",
    "cnn.add(tf.keras.layers.MaxPool2D(pool_size=2, #to set the size of the window used for pool\n",
    "                                  strides=2)) #to set the number of pixels by which the window shifts per iteration\n",
    "\n",
    "#Flattening\n",
    "\n",
    "cnn.add(tf.keras.layers.Flatten())\n",
    "\n",
    "#Full Connection\n",
    "\n",
    "cnn.add(tf.keras.layers.Dense(units=128, activation='relu')) #A higher number of neurons for more complex datas\n",
    "\n",
    "#Output Layer\n",
    "\n",
    "cnn.add(tf.keras.layers.Dense(units=OutputNeurons, activation='softmax')) #For non-binary classification"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/25\n",
      "4/4 [==============================] - 1s 155ms/step - loss: 0.9856 - accuracy: 0.5259 - val_loss: 1.0013 - val_accuracy: 0.4348\n",
      "Epoch 2/25\n",
      "4/4 [==============================] - 0s 97ms/step - loss: 0.8287 - accuracy: 0.6810 - val_loss: 0.8001 - val_accuracy: 0.8696\n",
      "Epoch 3/25\n",
      "4/4 [==============================] - 0s 97ms/step - loss: 0.7430 - accuracy: 0.7500 - val_loss: 0.6624 - val_accuracy: 0.8261\n",
      "Epoch 4/25\n",
      "4/4 [==============================] - 0s 105ms/step - loss: 0.5951 - accuracy: 0.8190 - val_loss: 0.6121 - val_accuracy: 0.8261\n",
      "Epoch 5/25\n",
      "4/4 [==============================] - 0s 101ms/step - loss: 0.4828 - accuracy: 0.8276 - val_loss: 0.4975 - val_accuracy: 0.8261\n",
      "Epoch 6/25\n",
      "4/4 [==============================] - 0s 95ms/step - loss: 0.4395 - accuracy: 0.8534 - val_loss: 0.4755 - val_accuracy: 0.8261\n",
      "Epoch 7/25\n",
      "4/4 [==============================] - 0s 99ms/step - loss: 0.3430 - accuracy: 0.8707 - val_loss: 0.4319 - val_accuracy: 0.8261\n",
      "Epoch 8/25\n",
      "4/4 [==============================] - 0s 96ms/step - loss: 0.2614 - accuracy: 0.9052 - val_loss: 0.6911 - val_accuracy: 0.7391\n",
      "Epoch 9/25\n",
      "4/4 [==============================] - 0s 110ms/step - loss: 0.2499 - accuracy: 0.9052 - val_loss: 0.4785 - val_accuracy: 0.7391\n",
      "Epoch 10/25\n",
      "4/4 [==============================] - 0s 100ms/step - loss: 0.2290 - accuracy: 0.8966 - val_loss: 0.5975 - val_accuracy: 0.7391\n",
      "Epoch 11/25\n",
      "4/4 [==============================] - 0s 94ms/step - loss: 0.1746 - accuracy: 0.9310 - val_loss: 0.6781 - val_accuracy: 0.7391\n",
      "Epoch 12/25\n",
      "4/4 [==============================] - 0s 99ms/step - loss: 0.1720 - accuracy: 0.9741 - val_loss: 0.7733 - val_accuracy: 0.7391\n",
      "Epoch 13/25\n",
      "4/4 [==============================] - 0s 95ms/step - loss: 0.1139 - accuracy: 0.9569 - val_loss: 0.8652 - val_accuracy: 0.7391\n",
      "Epoch 14/25\n",
      "4/4 [==============================] - 0s 111ms/step - loss: 0.1303 - accuracy: 0.9569 - val_loss: 0.8681 - val_accuracy: 0.7391\n",
      "Epoch 15/25\n",
      "4/4 [==============================] - 0s 110ms/step - loss: 0.0640 - accuracy: 0.9914 - val_loss: 0.8434 - val_accuracy: 0.7391\n",
      "Epoch 16/25\n",
      "4/4 [==============================] - 0s 94ms/step - loss: 0.0520 - accuracy: 0.9828 - val_loss: 0.5423 - val_accuracy: 0.8261\n",
      "Epoch 17/25\n",
      "4/4 [==============================] - 0s 102ms/step - loss: 0.0623 - accuracy: 1.0000 - val_loss: 0.9716 - val_accuracy: 0.7391\n",
      "Epoch 18/25\n",
      "4/4 [==============================] - 0s 94ms/step - loss: 0.0580 - accuracy: 0.9828 - val_loss: 0.9580 - val_accuracy: 0.6957\n",
      "Epoch 19/25\n",
      "4/4 [==============================] - 0s 99ms/step - loss: 0.0474 - accuracy: 0.9914 - val_loss: 0.9998 - val_accuracy: 0.6957\n",
      "Epoch 20/25\n",
      "4/4 [==============================] - 0s 98ms/step - loss: 0.0425 - accuracy: 0.9914 - val_loss: 1.2043 - val_accuracy: 0.6957\n",
      "Epoch 21/25\n",
      "4/4 [==============================] - 0s 105ms/step - loss: 0.0327 - accuracy: 1.0000 - val_loss: 1.3142 - val_accuracy: 0.7391\n",
      "Epoch 22/25\n",
      "4/4 [==============================] - 0s 96ms/step - loss: 0.0459 - accuracy: 0.9828 - val_loss: 0.7357 - val_accuracy: 0.7826\n",
      "Epoch 23/25\n",
      "4/4 [==============================] - 0s 96ms/step - loss: 0.0307 - accuracy: 0.9914 - val_loss: 0.8103 - val_accuracy: 0.7391\n",
      "Epoch 24/25\n",
      "4/4 [==============================] - 0s 110ms/step - loss: 0.0172 - accuracy: 1.0000 - val_loss: 1.0642 - val_accuracy: 0.7391\n",
      "Epoch 25/25\n",
      "4/4 [==============================] - 0s 97ms/step - loss: 0.0198 - accuracy: 1.0000 - val_loss: 1.1071 - val_accuracy: 0.7391\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x2a59a82f950>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Compiling CNN\n",
    "\n",
    "cnn.compile(optimizer = 'adam', #for binary\n",
    "            loss = 'categorical_crossentropy',\n",
    "            metrics = [\"accuracy\"])\n",
    "\n",
    "# Training the CNN on the Training set and evaluating it on the Test set\n",
    "\n",
    "cnn.fit(x = training_set, #Training set value\n",
    "        validation_data=test_set, #Test set value\n",
    "        epochs = 25) #No. of epochs"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Making Simple Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 face(s) cropped and saved in 'C:\\Drive D\\ArhatPersonal\\ML\\Practice\\Proj_FaceRecog\\dataset\\cropped_faces' folder.\n",
      "1/1 [==============================] - 0s 59ms/step\n",
      "Prediction is:  Anit\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from tensorflow.keras.preprocessing import image\n",
    "import clickNsave\n",
    "import sys\n",
    "\n",
    "img_path =  clickNsave.ClicknSave() # path of single input, '-1' if no faces detected, '0' if more than 1 face detected\n",
    "\n",
    "if img_path == '-1':\n",
    "    print('No faces detected')\n",
    "    sys.exit()\n",
    "\n",
    "elif img_path == '0':\n",
    "    print('More than 1 face detected')\n",
    "    sys.exit()\n",
    "\n",
    "test_image = image.load_img(img_path, # path of single input\n",
    "                            target_size = (64,64)) # resize to get size as trained\n",
    "test_image = image.img_to_array(test_image) # to convert it into a numpy array\n",
    "test_image = np.expand_dims(test_image, # to add extra dimension (for the batch number)\n",
    "                            axis = 0) # to put the batch number dimension as the first one\n",
    "\n",
    "result = cnn.predict(test_image)\n",
    "print('Prediction is: ', Mapping[np.argmax(result)])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# To add to dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sorry for the inconvenience. We will try to improve our model.\n"
     ]
    }
   ],
   "source": [
    "import shutil\n",
    "\n",
    "Q = input('Did we get it right? (y/n): ')\n",
    "if Q == 'y':\n",
    "    shutil.move(img_path, 'dataset/training_set/'+Mapping[np.argmax(result)])\n",
    "\n",
    "    import os\n",
    "\n",
    "    #Counting number of files in the destination folder\n",
    "\n",
    "    # folder path\n",
    "    dir_path = 'dataset/training_set/'+Mapping[np.argmax(result)]\n",
    "    count = 0\n",
    "    # Iterate directory\n",
    "    for path in os.listdir(dir_path):\n",
    "        # check if current path is a file\n",
    "        if os.path.isfile(os.path.join(dir_path, path)):\n",
    "            count += 1\n",
    "    print('File count:', count)\n",
    "\n",
    "    #Renaming the file to the next number\n",
    "\n",
    "    os.rename(dir_path+'/'+'face_0.jpg', dir_path+'/Added'+str(count)+'.png')\n",
    "\n",
    "    \n",
    "\n",
    "elif Q == 'n':\n",
    "    print('Sorry for the inconvenience. We will try to improve our model.')\n",
    "\n",
    "else:\n",
    "    print('Invalid input.')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
